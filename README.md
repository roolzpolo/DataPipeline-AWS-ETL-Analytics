# DataPipeline-AWS-ETL-Analytics

Este repositorio contiene todos los recursos y notebooks utilizados para desarrollar un pipeline completo de ingeniería de datos que utiliza Python, SQL, y AWS. El proyecto se centra en el diseño, implementación y análisis de un sistema transaccional y un data warehouse, seguido de un proceso ETL (Extract, Transform, Load) para alimentar la estructura de datos diseñada específicamente para responder a preguntas de negocio predefinidas.

Componentes del Proyecto

- Sistema Transaccional: Construcción automatizada utilizando AWS, boto3, y SQL.
- Ingestión de Datos: Población de la base de datos a través de scripts Python.
- Preguntas de Negocio: Definición de preguntas para orientar la creación del data warehouse.
- Arquitectura de Datos: Diseño de la estructura de datos (DDL) para responder a las preguntas de negocio.
- Proceso ETL: Uso de Python para migrar datos desde el sistema transaccional al data warehouse.
- Análisis de Datos: Exploración, análisis, y visualización de datos utilizando herramientas como pandas, matplotlib, y seaborn en Jupyter Notebook.

Tecnologías Utilizadas

- AWS (RDS, EC2)
- Python (boto3, pandas, numpy, matplotlib, seaborn)
- SQL
- Jupyter Notebook
- Objetivos del Proyecto

El objetivo principal de este proyecto es demostrar la capacidad de construir un pipeline de ingeniería de datos eficiente que no solo automatice la ingestión y el procesamiento de datos, sino que también facilite la toma de decisiones basada en el análisis profundo de datos. Este proyecto subraya la importancia de la ingeniería de datos y el análisis analítico en el entorno empresarial actual.
